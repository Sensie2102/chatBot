from connectDB import get_db
from pymongo import MongoClient
from langchain_community.vectorstores import MongoDBAtlasVectorSearch
import warnings
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_core.runnables import RunnableLambda
from langchain_huggingface import HuggingFaceEndpoint
from langchain_core.prompts import PromptTemplate
import os


token = os.getenv("HUGGING_FACE_TOKEN")

warnings.filterwarnings("ignore", category=UserWarning, module="langchain")


collection = get_db()

vectorStore = MongoDBAtlasVectorSearch(
    collection, HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2"), index_name="vector_index"
)

def retrieve_context(query,k=5):
    docs = vectorStore.max_marginal_relevance_search(query, k)
    context = []
    for doc in docs:
        context.append(doc.page_content)
    return "\n".join(context)

def generate_prompt(question):
    template = """You are a chatbot designed to answer the user's question based on the context provided to you.
Question: {question}
Documents:
{context}
Answer:"""
    prompt = PromptTemplate.from_template(template)
    formatted_prompt = prompt.format(question=question, context=retrieve_context(question,5))
    return formatted_prompt

repo_id = 'mistralai/Mistral-7B-Instruct-v0.3'




llm = HuggingFaceEndpoint(repo_id=repo_id,huggingfacehub_api_token=token,max_length=128,
    temperature=0.5)

prompt_runnable = RunnableLambda(lambda q: generate_prompt(q))

chain = prompt_runnable | llm

def get_chatbot_answer(user_question: str) -> str:
    """
    Given a user question, returns the answer generated by the
    prompt and LLM chain.
    """
    return chain.invoke(user_question)